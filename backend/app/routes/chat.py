# app/main.py (FastAPI backend) - Defines the /chat endpoint that streams responses token-by-token.
import asyncio

from fastapi import APIRouter, Depends, FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from .. import auth
from ..users import current_active_user
app = FastAPI()

router = APIRouter(prefix="/chat", tags=["chat"])


# Define the request body model
class ChatRequest(BaseModel):
    message: str


@router.post("/chat")
async def chat_endpoint(
    request: ChatRequest, current_user=Depends(current_active_user)
):
    """
    Accepts a user message and streams back a response token by token.
    Requires a valid authenticated user (via Depends on get_current_user).
    """
    user_message = request.message  # The message sent by the user

    # Simulate a streaming text generator (e.g., could call an AI model here)
    async def generate_response():
        # In a real scenario, connect to an AI service or generate response incrementally.
        # Here we'll simulate with a dummy response split into tokens.
        response_text = (
            f"AI response to '{user_message}'. This is a mock streaming reply."
        )
        for token in response_text.split():
            # Yield each token followed by a space
            yield token + " "
            await asyncio.sleep(0.1)  # simulate delay for streaming effect

    # Return a StreamingResponse that streams the content generated by the async generator
    return StreamingResponse(generate_response(), media_type="text/plain")
